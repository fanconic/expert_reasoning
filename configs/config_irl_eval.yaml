seed: 42

defaults:
   - override hydra/job_logging: default
   - override hydra/hydra_logging: default

model:
  checkpoint: 500
  name: "/mnt/pdata/caf83/tabular_reasoning/outputs/${wandb.run_name}/checkpoint-${model.checkpoint}"
  policy_name: "Qwen/Qwen2.5-7B-Instruct"
  reward_name: "Qwen/Qwen2.5-0.5B-Instruct" #Qwen/Qwen2.5-0.5B-Instruct
  max_prompt_length: 300
  max_completion_length: 824
  load_in_4bit: true
  fast_inference: false
  
  # Common settings
  lora_rank: 64  # Base LoRA rank
  use_gradient_checkpointing: true
  
  # Policy model specific
  policy_lora_rank: 128  # Optional: override base rank for policy
  policy_learning_rate: 1e-5
  add_expert_to_policy_optim: true
  add_expert_to_policy_balanced: false

  # Reward model specific  
  reward_lora_rank: 32  # Optional: override base rank for reward
  reward_learning_rate: 1e-5
  dense_rewards: true  # Whether to use dense rewards (token-level) or sparse (sequence-level)
  dense_gamma: 0.9
  advantage_calculation: "grpo"

dataset:
  name: "gsm8k_kd"   # Change to "gsm8k_kd" (or other) to switch datasets
  split: "test"

sampling:
  n_samples: 16
  temperature: 1
  top_p: 0.95

eval:
  ks : [1, 3, 5, 10]
  use_vllm: true
  per_device_eval_batch_size: 6
  report_to: "wandb"

wandb:
  project: "expert_reasoning"
  entity: eth_dlad_team32
  run_name: airl_per_wgan_09